<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Deep Learning // Study Guide</title>
<style>
    /* 
       --- STYLE CONFIGURATION --- 
       Terminal Green Ditherpunk Theme
    */
    :root {
        --bg-color: #000000;           /* Pure Black Background */
        --text-color: #00ff41;         /* Terminal Green Text */
        --accent-color: #00ff41;       /* Bright Green Accents */
        --dim-color: #003b00;          /* Dark Green for subtle borders */
        --border-color: #00ff41;       /* Green Borders */
        --font-main: 'Courier New', Courier, monospace;
        --font-header: 'Arial Black', Impact, sans-serif;
    }

    * { box-sizing: border-box; }

    body {
        margin: 0;
        padding: 0;
        background-color: var(--bg-color);
        color: var(--text-color);
        font-family: var(--font-main);
        line-height: 1.5;
        overflow-x: hidden;
    }

    /* --- DITHERPUNK VISUALS --- */
    
    .dither-layer {
        position: fixed;
        top: 0; left: 0; width: 100%; height: 100%;
        z-index: -1;
        background-image: radial-gradient(circle, #003b00 1px, transparent 1px);
        background-size: 4px 4px;
        opacity: 0.4;
    }

    .scanlines {
        position: fixed;
        top: 0; left: 0; width: 100%; height: 100%;
        background: linear-gradient(
            to bottom, 
            rgba(0, 255, 65, 0), 
            rgba(0, 255, 65, 0) 50%, 
            rgba(0, 20, 0, 0.2) 50%, 
            rgba(0, 20, 0, 0.2)
        );
        background-size: 100% 4px;
        pointer-events: none;
        z-index: 9999;
    }

    /* --- RESPONSIVE CONTAINER --- */
    .container {
        max-width: 900px;
        width: 100%;
        margin: 0 auto;
        padding: 40px 20px;
        border-left: 2px dashed var(--dim-color);
        border-right: 2px dashed var(--dim-color);
        background-color: rgba(0, 10, 0, 0.9);
        min-height: 100vh;
    }

    /* Typography */
    h1 {
        font-family: var(--font-header);
        text-transform: uppercase;
        font-size: 3rem;
        border-bottom: 5px solid var(--accent-color);
        margin-bottom: 40px;
        letter-spacing: -2px;
        text-shadow: 0px 0px 8px var(--accent-color);
        color: var(--accent-color);
        text-align: center;
        word-wrap: break-word;
    }

    h3 { margin-top: 0; color: var(--accent-color); text-transform: uppercase; }

    strong { color: var(--accent-color); text-decoration: underline; }
    em { font-style: normal; color: #50c878; border-bottom: 1px dotted var(--dim-color); }

    /* --- ACCORDION SYSTEM --- */

    /* PART LEVEL (Outer) */
    details.part {
        margin-bottom: 30px;
        border: 2px solid var(--border-color);
        background: #000;
        box-shadow: 6px 6px 0px var(--dim-color);
        transition: transform 0.1s;
    }
    
    details.part[open] {
        box-shadow: 4px 4px 0px var(--dim-color);
        transform: translate(2px, 2px);
    }

    details.part > summary {
        font-family: var(--font-header);
        font-size: 1.5rem;
        padding: 15px 20px;
        background-color: var(--accent-color);
        color: var(--bg-color);
        cursor: pointer;
        list-style: none;
        text-transform: uppercase;
        position: relative;
    }

    details.part > summary::-webkit-details-marker { display: none; }
    
    details.part > summary::after {
        content: '+'; 
        position: absolute; right: 20px; font-weight: 900;
    }
    details.part[open] > summary::after { content: '-'; }

    .part-content { padding: 20px; border-top: 2px solid var(--border-color); }
    
    .focus-line {
        display: block;
        font-size: 0.8rem;
        text-transform: uppercase;
        letter-spacing: 2px;
        margin-bottom: 20px;
        color: #008f11;
        font-weight: bold;
    }

    /* SECTION LEVEL (Middle) */
    details.section {
        margin-bottom: 15px;
        border: 1px solid var(--dim-color);
        background: #050505;
    }

    details.section > summary {
        font-family: var(--font-main);
        font-weight: bold;
        padding: 12px;
        background: #0a0a0a;
        color: var(--text-color);
        cursor: pointer;
        list-style: none;
        border-bottom: 1px solid transparent;
        text-transform: uppercase;
        font-size: 1.1rem;
    }

    details.section > summary:hover { 
        background: var(--dim-color); 
        color: var(--accent-color); 
    }
    details.section[open] > summary {
        border-bottom: 1px solid var(--dim-color);
        background: #0f0f0f;
        color: var(--accent-color);
        text-shadow: 0px 0px 5px var(--accent-color);
    }

    .section-content { padding: 20px; }

    /* SUBSECTION / CONTENT BLOCKS */
    .subsection {
        margin-bottom: 25px;
        border-left: 4px solid var(--dim-color);
        padding-left: 15px;
    }

    .subsection-title {
        background: var(--dim-color);
        color: var(--accent-color);
        padding: 2px 6px;
        font-weight: bold;
        text-transform: uppercase;
        display: inline-block;
        margin-bottom: 10px;
        font-size: 0.9rem;
    }

    p { margin-bottom: 12px; margin-top: 0; text-align: justify; }
    
    ul { padding-left: 20px; margin-bottom: 15px; }
    li { margin-bottom: 5px; }

    /* Code/Math styling */
    .code-block {
        background: #020a02;
        border: 1px dashed var(--dim-color);
        padding: 10px;
        margin: 10px 0;
        font-family: 'Courier New', monospace;
        color: var(--accent-color);
        overflow-x: auto;
        white-space: pre-wrap;
    }

    /* --- UI COMPONENTS --- */

    .dither-bg {
        background-image: 
            linear-gradient(45deg, var(--dim-color) 25%, transparent 25%), 
            linear-gradient(-45deg, var(--dim-color) 25%, transparent 25%), 
            linear-gradient(45deg, transparent 75%, var(--dim-color) 75%), 
            linear-gradient(-45deg, transparent 75%, var(--dim-color) 75%);
        background-size: 4px 4px;
    }

    .eye-btn {
        position: relative;
        width: 28px;
        height: 28px;
        background: #000;
        border: 1px solid var(--accent-color);
        cursor: pointer;
        padding: 4px;
        display: inline-flex;
        align-items: center;
        justify-content: center;
        margin-left: 8px;
        vertical-align: bottom;
        transition: transform 0.1s;
    }

    .eye-btn svg { width: 100%; height: 100%; fill: var(--accent-color); }

    .eye-btn:hover {
        background: var(--accent-color);
        transform: translate(-1px, -1px);
        box-shadow: 2px 2px 0px var(--dim-color);
    }
    .eye-btn:hover svg { fill: #000; }

    .retro-viewport {
        position: fixed;
        top: 50%; left: 50%;
        transform: translate(-50%, -50%);
        width: 80vw; height: 80vh;
        max-width: 900px; max-height: 700px;
        background-color: #000;
        border: 2px solid var(--accent-color);
        box-shadow: 0 0 50px rgba(0, 50, 0, 0.8);
        display: flex; flex-direction: column;
        z-index: 10000;
        visibility: hidden; opacity: 0;
        pointer-events: none;
        transition: opacity 0.2s;
        resize: both;
        overflow: hidden;
    }

    .retro-viewport.active {
        visibility: visible; opacity: 1; pointer-events: auto;
    }

    .vp-header {
        background: var(--accent-color);
        color: #000;
        padding: 5px 10px;
        font-weight: bold;
        font-family: var(--font-header);
        display: flex; justify-content: space-between;
        align-items: center;
        border-bottom: 2px solid #000;
        cursor: default;
    }

    .vp-close {
        background: #000; color: var(--accent-color);
        border: 1px solid #000; font-weight: 900; 
        cursor: pointer; font-family: var(--font-main);
    }
    .vp-close:hover { background: #fff; color: #000; }

    .vp-body { flex-grow: 1; position: relative; background: #000; }
    .vp-body iframe { width: 100%; height: 100%; border: none; }

    /* Loading Indicator */
    .loading-txt {
        animation: blink 1s infinite;
        color: var(--dim-color);
    }
    @keyframes blink { 0% {opacity: 1;} 50% {opacity: 0.5;} 100% {opacity: 1;} }

    @media (max-width: 600px) {
        h1 { font-size: 1.8rem; border-bottom-width: 3px; }
        details.part > summary { font-size: 1.1rem; padding: 12px; }
        details.section > summary { font-size: 0.9rem; }
        .container { padding: 10px; border: none; }
        .part-content, .section-content { padding: 10px; }
        p { text-align: left; }
        .retro-viewport { width: 95vw; height: 60vh; }
    }
</style>
</head>
<body>

<div class="dither-layer"></div>
<div class="scanlines"></div>

<div class="container">
    <h1>Neural Networks<br>Study Guide</h1>

    <!-- ============================================== -->
    <!-- PART I: Activation Functions -->
    <!-- ============================================== -->
    <details class="part">
        <summary>Part I: Activation Functions</summary>
        <div class="part-content" id="container-activation">
            <span class="focus-line">Focus: Non-Linearity, Sigmoid, Tanh, ReLU</span>
            
            <!-- Overview Section -->
            <details class="section" open>
                <summary>Overview</summary>
                <div class="section-content">
                    <p><strong>Activation Functions</strong></p>
                    <p>Activation functions introduce non-linearity into neural networks, allowing them to learn complex patterns. Without them, a deep network would behave like a single linear layer regardless of depth.</p>
                </div>
            </details>

            <!-- Sigmoid Section -->
            <details class="section">
                <summary>Sigmoid</summary>
                <div class="section-content">
                    <p>œÉ(x) = 1 / (1 + e‚ÅªÀ£)</p>
                    <p>Output range: [0, 1]</p>
                    <p>Historically used in output layers for binary classification (interpretable as probability).</p>
                    <p>Major drawbacks:</p>
                    <ul>
                        <li><strong>Vanishing gradient</strong>: gradients near 0 or 1 become tiny ‚Üí slow or stalled learning in early layers.</li>
                        <li>Outputs are not zero-centered ‚Üí can cause zig-zagging updates.</li>
                    </ul>
                </div>
            </details>

            <!-- Tanh Section -->
            <details class="section">
                <summary>Tanh (Hyperbolic Tangent)</summary>
                <div class="section-content">
                    <p>tanh(x) = (eÀ£ - e‚ÅªÀ£) / (eÀ£ + e‚ÅªÀ£)</p>
                    <p>Output range: [-1, 1]</p>
                    <p>Zero-centered (better than sigmoid for hidden layers) ‚Üí gradients more likely to point in useful directions.</p>
                    <p>Still suffers from <strong>vanishing gradient</strong> problem for large |x|.</p>
                </div>
            </details>

            <!-- ReLU Section -->
            <details class="section">
                <summary>ReLU (Rectified Linear Unit)</summary>
                <div class="section-content">
                    <p>f(x) = max(0, x)</p>
                    <p>Output range: [0, ‚àû)</p>
                    <p>Advantages:</p>
                    <ul>
                        <li>Very fast to compute (simple thresholding).</li>
                        <li>No vanishing gradient for positive values ‚Üí faster convergence in practice.</li>
                        <li>Promotes sparsity (many neurons output 0) ‚Üí efficient computation & regularization effect.</li>
                    </ul>
                    <p>Main issue: <strong>Dying ReLU</strong> problem ‚Äî neurons can get stuck outputting 0 forever if they receive only negative inputs (gradient = 0).</p>
                </div>
            </details>

            <!-- Leaky ReLU Section -->
            <details class="section">
                <summary>Leaky ReLU</summary>
                <div class="section-content">
                    <p>f(x) = max(Œ±x, x)   where Œ± is a small constant (typically 0.01)</p>
                    <p>Output range: (-‚àû, ‚àû)</p>
                    <p>Fixes dying ReLU by allowing a small, non-zero gradient when x < 0 ‚Üí keeps neurons "alive" and learning even when mostly inactive.</p>
                    <p>Variants: Parametric ReLU (PReLU ‚Äî Œ± is learned), ELU, GELU, Swish, etc., offer further improvements in some cases.</p>
                </div>
            </details>

            <!-- Summary Section -->
            <details class="section">
                <summary>Quick Comparison Summary</summary>
                <div class="section-content">
                    <ul>
                        <li><strong>Sigmoid</strong>: Good for binary output layers; avoid in hidden layers.</li>
                        <li><strong>Tanh</strong>: Better than sigmoid for hidden layers; still prone to vanishing gradients.</li>
                        <li><strong>ReLU</strong>: Default choice for most hidden layers ‚Äî fast & effective.</li>
                        <li><strong>Leaky ReLU / variants</strong>: Use when you suspect dying neurons or want smoother behavior on negatives.</li>
                    </ul>
                </div>
            </details>
        </div>
    </details>

    <!-- ============================================== -->
    <!-- PART II: Optimization & Learning Rates -->
    <!-- ============================================== -->
    <details class="part">
        <summary>Part II: Optimization & Learning Rates</summary>
        <div class="part-content" id="container-optimization">
            <span class="focus-line">Focus: Gradient Descent, Adam, Decay</span>
            
            <!-- Intro / Overview -->
            <details class="section" open>
                <summary>Overview: Core Concepts</summary>
                <div class="section-content">
                    <p><strong>Backpropagation, Gradient Descent, Chain Rule, Loss Functions, Optimization Schedules, Learning Rates</strong></p>
                    <div class="code-block">JUST USE ADAM LOL</div>
                    <p>Parameters are tuned by quantifying how "off" a model's predictions are [<strong>Loss Function</strong>], feeding the error backwards through layers [<strong>Backpropagation</strong>], using the <strong>Chain Rule of Differentiation</strong> to compute how each weight contributed to the loss, then adjusting weights via <strong>Gradient Descent</strong> to reduce future error.</p>
                </div>
            </details>

            <!-- Gradient Descent Variants -->
            <details class="section">
                <summary>Gradient Descent Variants</summary>
                <div class="section-content">
                    <p>The core idea: shift weights opposite the gradient direction to minimize loss.</p>
                </div>
            </details>

            <!-- Batch GD -->
            <details class="section">
                <summary>Batch Gradient Descent</summary>
                <div class="section-content">
                    <p>Most thorough but often impractical: compute gradient over the <strong>entire</strong> training dataset before one parameter update. Very accurate direction, smooth convergence, but slow and memory-heavy for large datasets.</p>
                </div>
            </details>

            <!-- SGD -->
            <details class="section">
                <summary>Stochastic Gradient Descent (SGD)</summary>
                <div class="section-content">
                    <p>Extreme opposite: process <strong>one</strong> training example at a time, compute its loss/gradient, update parameters immediately. Fast, low memory, noisy updates (helps escape local minima), but oscillates around minimum.</p>
                </div>
            </details>

            <!-- Mini-Batch -->
            <details class="section">
                <summary>Mini-Batch Gradient Descent</summary>
                <div class="section-content">
                    <p>Best practical compromise (most common in deep learning): use a small batch (e.g., 32‚Äì256 examples). Average gradient over the batch ‚Üí one update. Balances noise/speed/accuracy; enables efficient vectorization/GPU use.</p>
                </div>
            </details>

            <!-- AdaGrad -->
            <details class="section">
                <summary>AdaGrad (Adaptive Gradient)</summary>
                <div class="section-content">
                    <p>Adapts learning rate <strong>per parameter</strong>: parameters with large gradients get smaller updates (rare features), infrequent ones get larger. Accumulates squared gradients indefinitely ‚Üí learning rate decays to near-zero ‚Üí can "freeze" learning prematurely.</p>
                </div>
            </details>

            <!-- AdaDelta -->
            <details class="section">
                <summary>AdaDelta</summary>
                <div class="section-content">
                    <p>Fixes AdaGrad's aggressive decay: uses decaying average (sliding window) of squared gradients instead of infinite accumulation. Also tracks history of parameter updates (Œîw) to replace manual learning rate entirely. No global Œ∑ needed.</p>
                </div>
            </details>

            <!-- Adam -->
            <details class="section">
                <summary>Adam (Adaptive Moment Estimation)</summary>
                <div class="section-content">
                    <p>Default go-to optimizer in deep learning. Combines momentum + per-parameter adaptive rates. Maintains two moving averages per parameter:</p>
                    <ul>
                        <li><strong>First moment (m) ‚Äì Momentum</strong>: exponentially decaying average of gradients (helps accelerate in consistent directions, dampens oscillations).</li>
                        <li><strong>Second moment (v) ‚Äì Variance</strong>: exponentially decaying average of squared gradients (measures gradient magnitude/volatility; adapts step size).</li>
                    </ul>
                    <p>Update: m / ‚àöv (direction from momentum, scaled down in volatile/high-gradient areas). Includes bias correction for early training. Robust, works well with little tuning.</p>
                </div>
            </details>

            <!-- Chain Rule -->
            <details class="section">
                <summary>Chain Rule of Differentiation</summary>
                <div class="section-content">
                    <p>Core of backpropagation: computes how much a specific weight contributed to the total error by propagating derivatives backward through the computation graph.</p>
                </div>
            </details>

            <!-- Learning Rates -->
            <details class="section">
                <summary>Learning Rates</summary>
                <div class="section-content">
                    <p>Hyperparameter controlling step size. Too high ‚Üí overshoot/oscillate/diverge. Too low ‚Üí slow convergence or stuck in saddle points.</p>
                </div>
            </details>

            <!-- Optimization Schedules -->
            <details class="section">
                <summary>Optimization Schedules</summary>
                <div class="section-content">
                    <p>Strategies to dynamically adjust learning rate during training (e.g., step decay, exponential decay, cosine annealing, ReduceLROnPlateau) to improve convergence.</p>
                </div>
            </details>
        </div>
    </details>

    <!-- ============================================== -->
    <!-- PART III: Convolutional Neural Network -->
    <!-- ============================================== -->
    <details class="part">
        <summary>Part III: Convolutional Neural Network</summary>
        <div class="part-content" id="container-cnn">
            <span class="focus-line">Focus: Kernels, Padding, Pooling, Strides</span>
            
            <details class="section">
                <summary>1. DIFFERENT TYPES OF CNNs </summary>
                <div class="section-content">
                    <p><strong>CONVOLUTIONAL NEURAL NETWORKS</strong></p>
                    <p><strong>Spatial exploration based CNNs:</strong> Using different kernel sizes in order to explore different levels of visual features in input data.</p>
                    <p><strong>Depth Based CNNs:</strong> The depth here refers to the number of layers of the neural network. Shallow networks can only detect simple patterns like edges or curves. By stacking more layers, the network combines these simple patterns to recognize increasingly complex shapes‚Äîfrom textures and parts (like an eye or a wheel) to entire objects (like a face or a car).</p>
                    <p><strong>Width Based CNNs:</strong> Increasing the width (number of feature maps or channels) is useful when the model needs to capture a high diversity of fine-grained features at the same level of abstraction. While depth allows a model to understand what an object is (hierarchy), width determines how much detail the model can retain about the object.</p>
                    <p><strong>Multi-path-based CNNs:</strong> Deep layers are good at knowing what an object is but bad at knowing exactly where it is because the image size shrinks. Skip connections act as bridges, carrying high-resolution details from early layers directly to deeper ones. This allows the model to preserve fine details (like edges) while understanding the big picture.</p>
                    <p><strong>Example of Multi-path-based CNNs</strong></p>
                    <div class="subsection">
                        <span class="subsection-title">Aerospace</span>
                        <p>Satellite imagery involves analyzing vast landscapes where context and detail are equally important. For example, in disaster response after a hurricane, a model must understand the global context (a flooded region) while identifying specific local features (a washed-out bridge or a rooftop). Multi-path networks facilitate "multi-scale feature fusion." By connecting layers that represent different zoom levels, the model effectively sees the forest and the trees simultaneously. This allows for accurate land-use mapping (distinguishing similar-looking crops based on texture) and change detection, ensuring that small but critical changes on the ground are not ignored by the model‚Äôs high-level generalization.</p>
                    </div>
                    <p><strong>LAYER TYPES REVIEW</strong></p>
                    <p><strong>Convolution:</strong></p>
                    <p>You have a small grid of numbers (a "kernel") that slides across the image. At every stop, it performs a dot product; It asks: "How much does this patch match my specific pattern?" If the kernel is looking for a vertical line and finds one, the math returns a high number. If not, it returns a low one.</p>
                    <p><strong>Max Pooling:</strong></p>
                    <p>Invariance to translations.</p>
                    <p>Think of the previous layer (Convolution) as creating a "heat map" of where specific features are. If it was looking for a curve, it put a high number (like a 9) where it saw a perfect curve, and a low number (like a 0) where it didn't.</p>
                    <p><strong>Dense Layers</strong></p>
                    <p>Think of it as a funnel for reasoning. If you jump straight from raw evidence to the verdict, it's too chaotic. The first dense layer acts as a synthesizer: it combines scattered clues into high-level abstract concepts (like "upper loop" or "curved spine"). The second, smaller layer creates a bottleneck. By physically shrinking the layer size, we force the network to discard noise and compress those concepts into a concise summary before the final decision is delivered.</p>
                    <p>P.S. The typical number of dense layers is between 1 and 3.</p>
                </div>
            </details>

            <details class="section">
                <summary>2. ResNet Model</summary>
                <div class="section-content">
                    <p><strong>ResNet Model</strong></p>
                    <p><strong>RESIDUAL CONVOLUTIONAL BLOCK</strong></p>
                    <p>Residuals are the features that differentiate your target.</p>
                    <p><strong>Backpropagation Pass:</strong> The skip connection enables incredibly efficient backpropagation allowing the model to easily identify exactly which features (like texture vs. shape) need to be adjusted</p>
                    <p><strong>THE PROCESS OF "ADDING"</strong></p>
                    <p>'Adding' is used to enable the model to differentiate important features faster</p>
                    <p>1. The Skip Connection (ùê±): "I will carry the original information (the baseline) for you."</p>
                    <p>2. The Conv Layer (ùêÖ(ùê±)): "Great, since you are holding the baseline, I will calculate only the new features (the residual/the difference)."</p>
                    <p>3. The Addition (+): Combines them.</p>
                    <ul>
                        <li>The Job is Easier (Forward Pass): It is much faster for the model to learn "Just add the tank" (the residual) than it is to learn "Reconstruct the entire forest and the tank." Because the job is simpler, the model learns to identify those important features (the tank) much quicker.</li>
                        <li>The Feedback is Faster (Backward Pass): Because of the addition, the "Teacher" (the error signal) can shout directly at the early layers to tell them what they got wrong, without the message getting garbled by 50 layers in between.</li>
                    </ul>
                    <p><strong>CONVOLUTION VS IDENTITY BLOCK</strong></p>
                    <p>Convolutional block = shrink data,</p>
                    <p>Identity block = inspect data</p>
                    <p><strong>Identity Block</strong></p>
                    <p>The input has the same height, width, and number of channels as the output.</p>
                    <ul>
                        <li>When to use: When you want to make the network "deeper" without shrinking the image size or changing the feature count.</li>
                    </ul>
                    <p><strong>Convolutional Block</strong></p>
                    <p>The input has different dimensions than the output (usually the image gets smaller, or the number of filters/channels doubles).</p>
                    <ul>
                        <li>When to use: When you need to downsample the image (reduce resolution) or increase the number of features (channels) to capture more complex abstract concepts.</li>
                    </ul>
                    <p><strong>EXAMPLE FOR WHEN TO USE IDENTITY VS CONVOLUTIONAL BLOCKS</strong></p>
                    <div class="subsection">
                        <span class="subsection-title">Infrastructure (Pavement Crack Detection)</span>
                        <p>Automated road inspection using high-speed camera footage. Application:</p>
                        <ul>
                            <li><strong>Convolutional Block:</strong> You use this early in the network to aggressively downsample the massive 4K road images and double the channel depth (e.g., from 64 to 128 filters). The model shifts from looking at "gray pixels" to looking for "lines and gaps."</li>
                            <li><strong>Identity Block:</strong> Once you have a feature map of potential cracks, you stack multiple Identity blocks. These layers refine the understanding of "crack vs. tar seal" without shrinking the map further, ensuring you don't lose the spatial location of the crack which is needed for maintenance crews.</li>
                        </ul>
                    </div>
                </div>
            </details>

            <details class="section">
                <summary>3. DenseNet Model</summary>
                <div class="section-content">
                    <p><strong>DenseNet Model:</strong></p>
                    <p>"In DenseNet, skip connections are used via concatenation to hand a layer the collective feature history of the block. This ensures that:</p>
                    <p>1. The model has access to low-level features (edges/textures found in early layers) without them being 'washed out' or abstracted away by subsequent convolutions.</p>
                    <p>2. The model becomes highly parameter-efficient because it can reuse these earlier features rather than regenerating them, creating a strong gradient flow from the end of the network back to the start."</p>
                    <p><strong>THE HOW:</strong></p>
                    <p>In DenseNet, a layer (e.g., Layer 10) receives the collective feature history of every preceding layer (Input + Layers 1‚Äì9), not just the previous one. This enables Feature Reuse: later layers access low-level features (like edges) and high-level features (like shapes) simultaneously. The network need not relearn an "edge" deep in the model, as Layer 1 passes that specific finding directly forward.</p>
                    <p>Critically, DenseNet preserves this data via concatenation rather than addition. In ResNet, adding features is like mixing red and blue paint into purple‚Äîthe original inputs are blended and inseparable. DenseNet's concatenation is like stapling a red index card (input) next to a blue one (processing); the next layer sees both distinctly. Because features remain separate in the depth dimension, the model can explicitly choose between the "granular" details from early layers or the "abstract" concepts from later ones, ensuring no information is "washed out."</p>
                </div>
            </details>

            <details class="section">
                <summary>4. CNN-LSTM</summary>
                <div class="section-content">
                    <p><strong>CNN-LSTM</strong></p>
                    <p>How the CNN data gets turned into sequential data that the LSTM can process.</p>
                    <p>1. Time-Distributed CNN: The model does not process the sequence all at once. Instead, it applies the same CNN layer to Frame 1, then Frame 2, and so on, independently.</p>
                    <p>2. Flattening: For each frame, the CNN condenses the complex image data into a single 1D "feature vector" (number column).</p>
                    <p>3. Sequence Formation: If your input has 20 frames, the CNN outputs 20 distinct feature vectors.</p>
                    <p>4. The Hand-off: These 20 vectors are lined up in chronological order. This ordered list is what the LSTM ingests. The LSTM reads the vector for Time 1, updates its memory, then reads Time 2, allowing it to understand the flow of time.</p>
                    <p><strong>EXAMPLE</strong></p>
                    <div class="subsection">
                        <span class="subsection-title">Defense: Underwater Mine and Object Detection (Sonar)</span>
                        <p>Use Case: Identifying mines, submarines, or debris in side-scan sonar imagery.</p>
                        <p>How it works: Sonar data is noisy and difficult to interpret due to the complex scattering of sound waves underwater.</p>
                        <ul>
                            <li><strong>The CNN Component:</strong> Acts as a feature extractor on the raw sonar frames.it filters out noise (sea floor reverberation) and highlights spatial shapes that resemble man-made objects (edges, metallic textures, and shadows).</li>
                            <li><strong>The LSTM Component:</strong> Analyzes the sequence of these processed sonar pings as the vessel moves. Since a single ping might be ambiguous, the LSTM uses the temporal context‚Äîhow the object's shadow and highlight shift across multiple frames‚Äîto confirm the object's classification.</li>
                        </ul>
                        <p>Impact: This hybrid approach significantly reduces false positives caused by rocks or coral. It allows Autonomous Underwater Vehicles (AUVs) to clear minefields more safely and autonomously, keeping naval personnel out of hazardous zones.</p>
                    </div>
                </div>
            </details>
        </div>
    </details>

</div>

<!-- VISUALIZATION VIEWPORT CONTAINER -->
<!-- This modal allows you to display iframes/websites over the guide -->
<div id="viz-viewport" class="retro-viewport dither-bg">
    <div class="vp-header">
        <span id="vp-title">VISUALIZATION</span>
        <button class="vp-close" onclick="closeViewport()">[CLOSE_X]</button>
    </div>
    <div class="vp-body">
        <iframe id="vp-iframe" src=""></iframe>
    </div>
</div>

<script>
    // --- VISUALIZATION LOGIC ---
    // Controls the pop-up window. Use openViewport('url', 'title') in your buttons.
    const viewport = document.getElementById('viz-viewport');
    const iframe = document.getElementById('vp-iframe');
    const title = document.getElementById('vp-title');

    function openViewport(url, label) {
        iframe.src = url;
        title.innerText = label || "SYSTEM_VISUALIZER";
        viewport.classList.add('active');
    }

    function closeViewport() {
        viewport.classList.remove('active');
        // Clear src to stop animations/audio when closed
        setTimeout(() => { iframe.src = ""; }, 200);
    }
</script>

</body>
</html>